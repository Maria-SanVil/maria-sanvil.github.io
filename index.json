[{"authors":null,"categories":null,"content":"Hi! I'm a <strong>PhD student</strong> at Ropert: Robotics, Computer Vision and Artificial Intelligence group< at the University of Zaragoza (Unizar), Spain, supervised by Dr. Josechu J. Guerrero since 2022.\nPreviously I studied a Bachelor&rsquo;s degree in Industrial Technologies Engineering and a Master&rsquo;s degree in Industrial Engineering, both at Unizar where I started my career as a Computer Vision researcher.\nMy work revolves around <strong>Egocentric Vision</strong>, focusing on how it can enhance the way humans understand and interact with their surroundings.\nDownload my resumé.\n","date":1668816000,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1668816000,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Hi! I'm a <strong>PhD student</strong> at Ropert: Robotics, Computer Vision and Artificial Intelligence group< at the University of Zaragoza (Unizar), Spain, supervised by Dr. Josechu J. Guerrero since 2022.","tags":null,"title":"María Santos Villafranca","type":"authors"},{"authors":["Maria Santos-Villafranca*","Dustin Carrión-Ojeda*","Alejandro Perez-Yus","Jesus Bermudez-Cameo","Jose J. Guerrero","Simone Schaub-Meyer","","",""],"categories":null,"content":"","date":1668816000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668816000,"objectID":"6a2570a7ee7b4a704386956231dd26bc","permalink":"https://maria-sanvil.github.io/publication/karmma/","publishdate":"2022-11-19T00:00:00Z","relpermalink":"/publication/karmma/","section":"publication","summary":"Action recognition is an essential task in egocentric vision due to its wide range of applications across many fields. While deep learning methods have been proposed to address this task, most rely on a single modality, typically video. However, including additional modalities may improve the robustness of the approaches to common issues in egocentric videos, such as blurriness and occlusions. Recent efforts in multimodal egocentric action recognition often assume the availability of all modalities, leading to failures or performance drops when any modality is missing. To address this, we introduce an efficient multimodal knowledge distillation approach for egocentric <b>a</b>ction <b>r</b>ecognition that is <b>r</b>obust to <b>m</b>issing <b>m</b>od<b>a</b>lities (KARMMA) while still benefiting when multiple modalities are available. Our method focuses on resource-efficient development by leveraging pre-trained models as unimodal feature extractors in our teacher model, which distills knowledge into a much smaller and faster student model. Experiments on the Epic-Kitchens and Something-Something datasets demonstrate that our student model effectively handles missing modalities while reducing its accuracy drop in this scenario.","tags":null,"title":"Knowledge Distillation for Multimodal Egocentric Action Recognition Robust to Missing Modalities","type":"publication"},{"authors":["Alejandro Perez-Yus","Maria Santos-Villafranca","Julia Tomas-Barba","Jesus Bermudez-Cameo","Lorenzo Montano-Olivan","Gonzalo Lopez-Nicolas","Jose J. Guerrero"],"categories":null,"content":"","date":1651363200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651363200,"objectID":"78fffad3b1a17435d722972699f68635","permalink":"https://maria-sanvil.github.io/publication/raspv/","publishdate":"2022-05-01T00:00:00Z","relpermalink":"/publication/raspv/","section":"publication","summary":"One of the main challenges of visual prostheses is to augment the perceived information to improve the experience of its wearers. Given the limited access to implanted patients, in order to facilitate the experimentation of new techniques, this is often evaluated via Simulated Prosthetic Vision (SPV) with sighted people. In this work, we introduce a novel SPV framework and implementation that presents major advantages with respect to previous approaches. First, it is integrated into a robotics framework, which allows us to benefit from a wide range of methods and algorithms from the field (e.g. object recognition, obstacle avoidance, autonomous navigation, deep learning). Second, we go beyond traditional image processing with 3D point clouds processing using an RGB-D camera, allowing us to robustly detect the floor, obstacles and the structure of the scene. Third, it works either with a real camera or in a virtual environment, which gives us endless possibilities for immersive experimentation through a head-mounted display. Fourth, we incorporate a validated temporal phosphene model that replicates time effects into the generation of visual stimuli. Finally, we have proposed, developed and tested several applications within this framework, such as avoiding moving obstacles, providing a general understanding of the scene, staircase detection, helping the subject to navigate an unfamiliar space, and object and person detection. We provide experimental results in real and virtual environments.","tags":null,"title":"RASPV: A Robotics Framework for Augmented Simulated Prosthetic Vision","type":"publication"},{"authors":["María Santos Villafranca*","Bruno Berenguel-Baeta*","Jesus Bermudez-Cameo","Alejandro Perez-Yus","Jose J. Guerrero"],"categories":null,"content":"","date":1646092800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646092800,"objectID":"e278c88545c5b9891de5ed98fcebbdc8","permalink":"https://maria-sanvil.github.io/publication/fisheye/","publishdate":"2022-03-01T00:00:00Z","relpermalink":"/publication/fisheye/","section":"publication","summary":"Convolution kernels are the basic structural component of convolutional neural networks (CNNs). In the last years there has been a growing interest in fisheye cameras for many applications. However, the radially symmetric projection model of these cameras produces high distortions that affect the performance of CNNs, especially when the field of view is very large. In this work, we tackle this problem by proposing a method that leverages the calibration of cameras to deform the convolution kernel accordingly and adapt to the distortion. That way, the receptive field of the convolution is similar to standard convolutions in perspective images, allowing us to take advantage of pre-trained networks in large perspective datasets. We show how, with just a brief fine-tuning stage in a small dataset, we improve the performance of the network for the calibrated fisheye with respect to standard convolutions in depth estimation and semantic segmentation.","tags":null,"title":"Convolution kernel adaptation to calibrated fisheye","type":"publication"},{"authors":["María Santos Villafranca","Julia Tomas-Barba","Alejandro Perez-Yus","Jesus Bermudez-Cameo","Jose J. Guerrero"],"categories":null,"content":"","date":1631664000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631664000,"objectID":"6be824f0faaa9c916ad8bb74313e2318","permalink":"https://maria-sanvil.github.io/publication/automatica/","publishdate":"2021-09-15T00:00:00Z","relpermalink":"/publication/automatica/","section":"publication","summary":"Recent advancements have demonstrated the potential of visual prostheses in partially restoring vision for certain types of visual impairment. However, due to their limitations, there is a growing interest in developing computer vision techniques to extract pertinent information from the surroundings and adapt it for use with these prostheses. Evaluating the effectiveness of such methods is challenging due to the scarcity of individuals who have undergone the implantation procedure. To overcome this, visual prosthesis simulators are being used, enabling experimentation with individuals possessing healthy vision. This research introduces a novel and immersive simulator integrated within a robotics framework, specifically designed for testing various modes of representation using virtual reality goggles. Notably, this simulator incorporates a temporal model inspired by real patient experiments, emphasizing the time dimension in generating visual stimuli. Additionally, the simulator offers complete user immersion within a virtual environment that incorporates a semantic segmentation neural network, aiding in object and person detection.","tags":null,"title":"Simulador inmersivo de vision protésica modelando estímulos espacio-temporales","type":"publication"},{"authors":["María Santos Villafranca","Alejandro Perez-Yus","Jesus Bermudez-Cameo","Jose J. Guerrero"],"categories":null,"content":"","date":1602720000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602720000,"objectID":"4d3e8bdf34640061b255946f5911e1d8","permalink":"https://maria-sanvil.github.io/publication/jovenes/","publishdate":"2020-10-15T00:00:00Z","relpermalink":"/publication/jovenes/","section":"publication","summary":"Recent advances in visual prostheses have shown that it is possible to restore part of the vision in certain cases of visual impairment, albeit with limitations such as low resolution or a reduced field of view. In order to experiment more easily, simulated prosthetic vision (SPV) is used for easier experimentation.","tags":null,"title":"Sistema de realidad virtual para exploración 3D con visión protésica simulada.","type":"publication"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"e8f8d235e8e7f2efd912bfe865363fc3","permalink":null,"publishdate":"2016-04-27T00:00:00Z","relpermalink":"https://i3a.unizar.es/es/proyectos/divad","section":"project","summary":"Bayesian deep learning for dynamic interactions applied to visual assistive devices. ","tags":null,"title":"Aprendizaje profundo bayesiano para interacciones dinámicas aplicadas a dispositivos asistenciales visuales. ","type":"project"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"e8f8d235e8e7f2efd912bfe865363fc3","permalink":null,"publishdate":"2016-04-27T00:00:00Z","relpermalink":"https://i3a.es/en/projects/omnidirectional-vision-man-made-scene-understanding","section":"project","summary":"OVIMSU","tags":null,"title":"Omnidirectional Vision for Man-Made Scene Understanding, OVIMSU.","type":"project"}]